{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "722e1e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n",
      "\n",
      "**Sources:**\n",
      "- 1.DOE-Manual-for-Procurement-of-Goods.pdf (Page: 80)\n",
      "- 1.DOE-Manual-for-Procurement-of-Goods.pdf (Page: 67)\n",
      "- 1.DOE-Manual-for-Procurement-of-Goods.pdf (Page: 30)\n",
      "- 1.DOE-Manual-for-Procurement-of-Goods.pdf (Page: 365)\n",
      "- 1.DOE-Manual-for-Procurement-of-Goods.pdf (Page: 79)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_neo4j import Neo4jChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Use the ABSOLUTE path to your vector database\n",
    "CHROMA_PATH = r\"C:\\Users\\shreyas\\Aerothon\\expirements\\db_root\\public\\Knowledge_vectors\"\n",
    "MODEL_NAME = \"llama3.1:8b-instruct-q4_K_M\"\n",
    "\n",
    "# Standard local Ollama port is 11434\n",
    "OLLAMA_BASE_URL = \"http://127.0.0.1:11434\"\n",
    "\n",
    "# Neo4j Credentials\n",
    "NEO4J_URI = \"neo4j://localhost:7687\"\n",
    "NEO4J_AUTH = (\"neo4j\", \"password\")\n",
    "\n",
    "# Initialize Local AI Components with explicit base_url to fix ResponseError\n",
    "llm = ChatOllama(\n",
    "    model=MODEL_NAME, \n",
    "    temperature=0, \n",
    "    num_ctx=8192, \n",
    "    base_url=OLLAMA_BASE_URL\n",
    ")\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\", \n",
    "    base_url=OLLAMA_BASE_URL\n",
    ")\n",
    "\n",
    "# --- 2. SILENCE WARNINGS (Schema Initialization) ---\n",
    "def initialize_neo4j_schema():\n",
    "    \"\"\"Defines relationship types in Neo4j to prevent 'UnknownRelationship' warnings.\"\"\"\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n",
    "    with driver.session() as session:\n",
    "        session.run(\"\"\"\n",
    "            MERGE (s:Session {id: 'schema_init'})\n",
    "            MERGE (m:Message {content: 'init', role: 'system'})\n",
    "            MERGE (s)-[:LAST_MESSAGE]->(m)\n",
    "            MERGE (m)-[:NEXT]->(m)\n",
    "            DETACH DELETE s, m\n",
    "        \"\"\")\n",
    "    driver.close()\n",
    "\n",
    "initialize_neo4j_schema()\n",
    "\n",
    "# --- 3. CONNECT TO DATABASES ---\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=CHROMA_PATH, \n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"Knowledge_Store\"  # Must match your ingestion script\n",
    ")\n",
    "# Search k=5 chunks (smaller chunks = more relevant segments)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# --- 4. PROMPTS & LOGIC ---\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "context_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Given the history, rephrase the user's last question into a standalone question.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer ONLY using the context below. If unsure, say you don't know.\\n\\nContext:\\n{context}\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "def get_query(input_data):\n",
    "    if input_data.get(\"chat_history\") and len(input_data[\"chat_history\"]) > 0:\n",
    "        chain = context_q_prompt | llm | StrOutputParser()\n",
    "        return chain.invoke(input_data)\n",
    "    return input_data[\"input\"]\n",
    "\n",
    "# The core RAG Chain\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=RunnableLambda(get_query) | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# --- 5. THE HYBRID CHAT FUNCTION ---\n",
    "def chat(user_input: str, session_id: str = \"user_session_1\"):\n",
    "    # Connect to persistent history in Neo4j\n",
    "    history = Neo4jChatMessageHistory(\n",
    "        url=NEO4J_URI,\n",
    "        username=NEO4J_AUTH[0],\n",
    "        password=NEO4J_AUTH[1],\n",
    "        session_id=session_id\n",
    "    )\n",
    "    \n",
    "    # 1. Generate standalone query based on history\n",
    "    smart_query = get_query({\"input\": user_input, \"chat_history\": history.messages})\n",
    "    \n",
    "    # 2. Retrieve most relevant documents\n",
    "    docs = retriever.invoke(smart_query)\n",
    "    \n",
    "    # 3. Generate response using the RAG chain\n",
    "    response = rag_chain.invoke({\"input\": user_input, \"chat_history\": history.messages})\n",
    "    \n",
    "    # 4. Update memory in Neo4j\n",
    "    history.add_user_message(user_input)\n",
    "    history.add_ai_message(response.content)\n",
    "    \n",
    "    # 5. Extract metadata for citations\n",
    "    sources = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        pg = doc.metadata.get(\"page\", \"N/A\")\n",
    "        sources.append(f\"- {src} (Page: {pg})\")\n",
    "    \n",
    "    return f\"{response.content}\\n\\n**Sources:**\\n\" + \"\\n\".join(set(sources))\n",
    "\n",
    "# --- TEST ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(chat(\"can you summarize Chapter 2: Need Assessment, Formulation of Specifications and Procurement Planning .?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd7d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
