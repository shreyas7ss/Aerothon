{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "722e1e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, **\"Coercive Practice\"** refers to any coercion or threat that impairs or harms a party or its property to influence the procurement process or affect the execution of a contract.\n",
      "\n",
      "In the context of procurement, coercive practices can take many forms, including:\n",
      "\n",
      "* Threats to withhold payment or benefits\n",
      "* Threats to cancel contracts or terminate agreements\n",
      "* Physical intimidation or harassment\n",
      "* Economic coercion, such as threats to harm a party's business or reputation\n",
      "\n",
      "Coercive practices are considered unethical and may be in violation of procurement regulations. They can undermine the integrity of the procurement process and create an unfair advantage for one party over another.\n",
      "\n",
      "**Reference:** The text states: \"**“Coercive practice”**: any coercion or any threat to impair or harm, directly or indirectly, any party or its property to influence the procurement process or affect the execution of a contract.\"\n",
      "\n",
      "**Sources:**\n",
      "- 1.DOE-Manual-for-Procurement-of-Goods.pdf (Page: 124)\n",
      "- 1.DOE-Manual-for-Procurement-of-Goods.pdf (Page: 82)\n",
      "- 1.DOE-Manual-for-Procurement-of-Goods.pdf (Page: 83)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_neo4j import Neo4jChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Use the ABSOLUTE path to your vector database\n",
    "CHROMA_PATH = r\"C:\\Users\\shreyas\\Aerothon\\expirements\\db_root\\public\\Knowledge_vectors\"\n",
    "MODEL_NAME = \"llama3.1:8b-instruct-q4_K_M\"\n",
    "\n",
    "# Standard local Ollama port is 11434\n",
    "OLLAMA_BASE_URL = \"http://127.0.0.1:11434\"\n",
    "\n",
    "# Neo4j Credentials\n",
    "NEO4J_URI = \"neo4j://localhost:7687\"\n",
    "NEO4J_AUTH = (\"neo4j\", \"password\")\n",
    "\n",
    "# Initialize Local AI Components with explicit base_url to fix ResponseError\n",
    "llm = ChatOllama(\n",
    "    model=MODEL_NAME, \n",
    "    temperature=0, \n",
    "    num_ctx=8192, \n",
    "    base_url=OLLAMA_BASE_URL\n",
    ")\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\", \n",
    "    base_url=OLLAMA_BASE_URL\n",
    ")\n",
    "\n",
    "# --- 2. SILENCE WARNINGS (Schema Initialization) ---\n",
    "def initialize_neo4j_schema():\n",
    "    \"\"\"Defines relationship types in Neo4j to prevent 'UnknownRelationship' warnings.\"\"\"\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n",
    "    with driver.session() as session:\n",
    "        session.run(\"\"\"\n",
    "            MERGE (s:Session {id: 'schema_init'})\n",
    "            MERGE (m:Message {content: 'init', role: 'system'})\n",
    "            MERGE (s)-[:LAST_MESSAGE]->(m)\n",
    "            MERGE (m)-[:NEXT]->(m)\n",
    "            DETACH DELETE s, m\n",
    "        \"\"\")\n",
    "    driver.close()\n",
    "\n",
    "initialize_neo4j_schema()\n",
    "\n",
    "# --- 3. CONNECT TO DATABASES ---\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=CHROMA_PATH, \n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"Knowledge_Store\"  # Must match your ingestion script\n",
    ")\n",
    "# Search k=5 chunks (smaller chunks = more relevant segments)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# --- 4. PROMPTS & LOGIC ---\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "context_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Given the history, rephrase the user's last question into a standalone question.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a professional Document Analyst. Use the provided context to answer questions precisely or provide comprehensive summaries.\n",
    "\n",
    "    GUIDELINES:\n",
    "    1. FACT RETRIEVAL: If the user asks for a specific fact, provide a direct answer grounded in the context.\n",
    "    2. SUMMARIZATION: If the user asks for a summary (e.g., 'Summarize Chapter 2'), synthesize all relevant details from the provided chunks into a structured overview. Use bullet points for key sub-topics.\n",
    "    3. INTEGRITY: Use ONLY the provided context. If the context does not contain the information requested, state that the materials are insufficient.\n",
    "    4. CITATION: Always mention which document or section your information comes from if available.\n",
    "\n",
    "    Context:\n",
    "    {context}\"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "def get_query(input_data):\n",
    "    if input_data.get(\"chat_history\") and len(input_data[\"chat_history\"]) > 0:\n",
    "        chain = context_q_prompt | llm | StrOutputParser()\n",
    "        return chain.invoke(input_data)\n",
    "    return input_data[\"input\"]\n",
    "\n",
    "# The core RAG Chain\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=RunnableLambda(get_query) | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# --- 5. THE HYBRID CHAT FUNCTION ---\n",
    "def chat(user_input: str, session_id: str = \"user_session_1\"):\n",
    "    # Connect to persistent history in Neo4j\n",
    "    history = Neo4jChatMessageHistory(\n",
    "        url=NEO4J_URI,\n",
    "        username=NEO4J_AUTH[0],\n",
    "        password=NEO4J_AUTH[1],\n",
    "        session_id=session_id\n",
    "    )\n",
    "    \n",
    "    # 1. Generate standalone query based on history\n",
    "    smart_query = get_query({\"input\": user_input, \"chat_history\": history.messages})\n",
    "    \n",
    "    # 2. Retrieve most relevant documents\n",
    "    docs = retriever.invoke(smart_query)\n",
    "    \n",
    "    # 3. Generate response using the RAG chain\n",
    "    response = rag_chain.invoke({\"input\": user_input, \"chat_history\": history.messages})\n",
    "    \n",
    "    # 4. Update memory in Neo4j\n",
    "    history.add_user_message(user_input)\n",
    "    history.add_ai_message(response.content)\n",
    "    \n",
    "    # 5. Extract metadata for citations\n",
    "    sources = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        pg = doc.metadata.get(\"page\", \"N/A\")\n",
    "        sources.append(f\"- {src} (Page: {pg})\")\n",
    "    \n",
    "    return f\"{response.content}\\n\\n**Sources:**\\n\" + \"\\n\".join(set(sources))\n",
    "\n",
    "# --- TEST ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(chat(\"Explain coercive practice with reference to procurement.?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fd7d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, I don't have any specific text or guidelines related to \"coercive practice\" in the context of procurement. However, based on general knowledge, I can provide a summary of what coercive practices might entail in a procurement setting.\n",
      "\n",
      "**Coercive Practices in Procurement:**\n",
      "\n",
      "Coercive practices refer to actions that use pressure, intimidation, or manipulation to influence the outcome of a procurement process. These practices can compromise the integrity and fairness of the procurement process, potentially leading to biased decisions.\n",
      "\n",
      "Some examples of coercive practices in procurement include:\n",
      "\n",
      "* **Undue influence**: Using personal relationships, favors, or other forms of leverage to sway decision-makers.\n",
      "* **Bribery**: Offering or accepting bribes to secure a contract or favorably influence the outcome of a procurement process.\n",
      "* **Unfair treatment**: Failing to provide equal opportunities for all bidders or treating certain suppliers unfairly.\n",
      "\n",
      "**Key Considerations:**\n",
      "\n",
      "To prevent coercive practices in procurement, organizations should:\n",
      "\n",
      "* Establish clear policies and procedures for procurement\n",
      "* Ensure transparency throughout the procurement process\n",
      "* Implement robust checks and balances to prevent undue influence\n",
      "* Foster a culture of integrity and accountability among procurement staff\n",
      "\n",
      "Please note that this summary is based on general knowledge and may not be exhaustive. If you have specific guidelines or context related to coercive practices in procurement, I would be happy to provide a more detailed explanation.\n",
      "\n",
      "**Sources:**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Updated to your new secure_DB folder\n",
    "CHROMA_PATH = r\"C:\\Users\\shreyas\\Aerothon\\expirements\\secure_DB\\public\\Knowledge_vectors\"\n",
    "MODEL_NAME = \"llama3.1:8b-instruct-q4_K_M\"\n",
    "OLLAMA_BASE_URL = \"http://127.0.0.1:11434\"\n",
    "\n",
    "# Initialize Local AI Components\n",
    "llm = ChatOllama(\n",
    "    model=MODEL_NAME, \n",
    "    temperature=0, \n",
    "    num_ctx=8192, \n",
    "    base_url=OLLAMA_BASE_URL\n",
    ")\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\", \n",
    "    base_url=OLLAMA_BASE_URL\n",
    ")\n",
    "\n",
    "# --- 2. CONNECT TO VECTOR STORE ---\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=CHROMA_PATH, \n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"Knowledge_Store\"\n",
    ")\n",
    "# Search k=7 chunks for better summary coverage\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    "# --- 3. PROMPTS & LOGIC ---\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "context_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Given the history, rephrase the user's last question into a standalone question.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a professional Document Analyst. Use the provided context to answer questions precisely or provide comprehensive summaries.\n",
    "\n",
    "    GUIDELINES:\n",
    "    1. FACT RETRIEVAL: Provide direct answers grounded in the context.\n",
    "    2. SUMMARIZATION: Synthesize relevant details into a structured overview. Use bullet points for sub-topics.\n",
    "    3. INTEGRITY: Use ONLY the provided context. If information is missing, state that the materials are insufficient.\n",
    "    4. CITATION: Mention source and page numbers.\n",
    "\n",
    "    Context:\n",
    "    {context}\"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "def get_query(input_data):\n",
    "    if input_data.get(\"chat_history\") and len(input_data[\"chat_history\"]) > 0:\n",
    "        chain = context_q_prompt | llm | StrOutputParser()\n",
    "        return chain.invoke(input_data)\n",
    "    return input_data[\"input\"]\n",
    "\n",
    "# The core RAG Chain\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=RunnableLambda(get_query) | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# --- 4. THE CHAT FUNCTION ---\n",
    "# Using a local list instead of Neo4j\n",
    "chat_history = [] \n",
    "\n",
    "def chat(user_input: str):\n",
    "    # 1. Generate standalone query based on history\n",
    "    smart_query = get_query({\"input\": user_input, \"chat_history\": chat_history})\n",
    "    \n",
    "    # 2. Retrieve most relevant documents\n",
    "    docs = retriever.invoke(smart_query)\n",
    "    \n",
    "    # 3. Generate response\n",
    "    response = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
    "    \n",
    "    # 4. Extract metadata\n",
    "    sources = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        pg = doc.metadata.get(\"page\", \"N/A\")\n",
    "        sources.append(f\"- {src} (Page: {pg})\")\n",
    "    \n",
    "    # 5. Update local memory\n",
    "    chat_history.extend([\n",
    "        HumanMessage(content=user_input), \n",
    "        AIMessage(content=response.content)\n",
    "    ])\n",
    "    \n",
    "    return f\"{response.content}\\n\\n**Sources:**\\n\" + \"\\n\".join(set(sources))\n",
    "\n",
    "# --- TEST ---\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Explain coercive practice with reference to procurement.?\"\n",
    "    print(chat(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd3fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
